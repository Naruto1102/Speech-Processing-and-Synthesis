{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67-K4SqzVjyP"
      },
      "source": [
        "\n",
        "\n",
        "# Speech Commands Classification - Lab Evaluation\n",
        "\n",
        "\n",
        "\n",
        "### Tasks:\n",
        "1. Summarize the paper in about 50 words.\n",
        "2. Download, analyze, and statistically describe the dataset.\n",
        "3. Train a classifier to distinguish commands.\n",
        "4. Report performance results using standard benchmarks.\n",
        "5. Record 30 samples of each command in your voice and create a new dataset.\n",
        "6. Fine-tune the classifier on your voice.\n",
        "7. Report the results.\n",
        "\n",
        "## 1. Paper Summary\n",
        "\n",
        "The research paper describes Google's Speech Commands dataset for training and evaluating keyword spotting systems. It targets simple speech recognition tasks, detecting spoken words with limited vocabulary. The paper covers data collection, challenges in building small on-device models, and provides baseline results, highlighting the dataset's utility for improving voice interface technology."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsVDXSTdWLZ1"
      },
      "source": [
        "## 2. Analyze Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FdyOuBQYVRq2"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Download and Analyze the Dataset\n",
        "import os\n",
        "import torchaudio\n",
        "from collections import Counter\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6TD1BbWwV6Pp"
      },
      "outputs": [],
      "source": [
        "# Create the data directory if it doesn't exist\n",
        "data_dir = './data'\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIZA4kbOV_rJ",
        "outputId": "68251e56-2a1a-4226-e51d-1a78fda9ada2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2.26G/2.26G [00:19<00:00, 124MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the Speech Commands dataset\n",
        "dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_dir, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7mmyB28qWCYt"
      },
      "outputs": [],
      "source": [
        "# Select 10 commands to work with\n",
        "selected_commands = ['yes', 'no', 'up', 'down', 'left', 'right', 'go', 'stop', 'on', 'off']\n",
        "\n",
        "# Limit the number of samples per command (e.g., 100 samples per command)\n",
        "samples_per_command = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLsG5in0WFl7",
        "outputId": "e03c06b2-2a93-4e32-c064-95eb5744e58a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample counts in subset: Counter({'down': 100, 'go': 100, 'left': 100, 'no': 100, 'off': 100, 'on': 100, 'right': 100, 'stop': 100, 'up': 100, 'yes': 100})\n",
            "Total subset size: 1000\n"
          ]
        }
      ],
      "source": [
        "# Create a subset of the dataset by filtering for the selected commands\n",
        "subset_indices = []\n",
        "command_counter = Counter()\n",
        "\n",
        "for idx, sample in enumerate(dataset):\n",
        "    label = sample[2]\n",
        "    if label in selected_commands and command_counter[label] < samples_per_command:\n",
        "        subset_indices.append(idx)\n",
        "        command_counter.update([label])\n",
        "\n",
        "    # Stop when we have enough samples for each command\n",
        "    if all(command_counter[cmd] >= samples_per_command for cmd in selected_commands):\n",
        "        break\n",
        "\n",
        "# Create a subset of the dataset\n",
        "subset_dataset = Subset(dataset, subset_indices)\n",
        "\n",
        "# Check the sample count for each command in the subset\n",
        "print(f\"Sample counts in subset: {command_counter}\")\n",
        "print(f\"Total subset size: {len(subset_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0JTonRtW6nt"
      },
      "source": [
        "## 3. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9hf5wYt0VSMN"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Data Preprocessing (Padding and Truncating)\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "g9T7BVULWfr_"
      },
      "outputs": [],
      "source": [
        "# Define a fixed length for all audio samples (1 second = 16000 samples at 16kHz)\n",
        "fixed_length = 16000\n",
        "\n",
        "# Custom collate function to pad and truncate audio data\n",
        "def collate_fn(batch):\n",
        "    waveforms = []\n",
        "    labels = []\n",
        "\n",
        "    for item in batch:\n",
        "        waveform = item[0]\n",
        "        label = item[2]\n",
        "\n",
        "        if waveform.shape[1] > fixed_length:\n",
        "            waveform = waveform[:, :fixed_length]\n",
        "        elif waveform.shape[1] < fixed_length:\n",
        "            pad_amount = fixed_length - waveform.shape[1]\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
        "\n",
        "        waveforms.append(waveform)\n",
        "        labels.append(label)\n",
        "\n",
        "    waveforms = torch.stack(waveforms)\n",
        "    return waveforms, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "P5gI2cizWijE"
      },
      "outputs": [],
      "source": [
        "# DataLoader for the subset dataset\n",
        "loader = DataLoader(subset_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHqsVkRBXEUi"
      },
      "source": [
        "## 4. CNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3eSGNqXW0WH",
        "outputId": "20de1ddb-d85c-497b-c8b6-a75f4620f716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 3.029912658035755\n",
            "Epoch 2/10, Loss: 1.9660861305892467\n",
            "Epoch 3/10, Loss: 1.441526371985674\n",
            "Epoch 4/10, Loss: 0.9779491610825062\n",
            "Epoch 5/10, Loss: 0.705291461199522\n",
            "Epoch 6/10, Loss: 0.6768102450296283\n",
            "Epoch 7/10, Loss: 0.8007653304375708\n",
            "Epoch 8/10, Loss: 0.3804143578745425\n",
            "Epoch 9/10, Loss: 0.23944445000961423\n",
            "Epoch 10/10, Loss: 0.18713378009852022\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Define and Train a CNN Classifier (with correct fully connected layer input size)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio.transforms as transforms\n",
        "\n",
        "# Define the MelSpectrogram transform to convert audio waveforms into spectrograms\n",
        "mel_spectrogram = transforms.MelSpectrogram(\n",
        "    sample_rate=16000, n_mels=128, n_fft=400, hop_length=160\n",
        ")\n",
        "\n",
        "# Define a simple CNN model for speech command classification\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(32 * 25 * 32, 128)  # Correct input size based on shape (32*25=800)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Get the number of classes (commands)\n",
        "num_classes = len(selected_commands)\n",
        "\n",
        "# Create a dictionary to map commands (labels) to numerical values\n",
        "label_to_index = {label: idx for idx, label in enumerate(selected_commands)}\n",
        "\n",
        "# Function to convert string labels to numerical indices\n",
        "def label_to_tensor(label):\n",
        "    return torch.tensor(label_to_index[label])\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleCNN(num_classes=num_classes).to('cuda')\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop for 10 epochs\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for waveforms, labels in loader:\n",
        "        # Convert waveforms to spectrograms\n",
        "        waveforms = mel_spectrogram(waveforms)\n",
        "        waveforms = waveforms.squeeze(1).unsqueeze(1).to('cuda')  # Remove extra dimension, add channel dimension\n",
        "        labels = torch.tensor([label_to_tensor(label) for label in labels]).to('cuda')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(waveforms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(loader)}')\n",
        "\n",
        "print('Training completed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzvfL4tuXIcl"
      },
      "source": [
        "## 5. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfrQ_hlLVaH5",
        "outputId": "37403c69-fbd3-434b-ae12-376551aa23fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 98.4%\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Evaluate the Model\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Path to your recordings folder in Drive\n",
        "recordings_path = '/content/drive/My Drive/path_to_recordings_folder'\n",
        "\n",
        "# Verify the recordings are accessible\n",
        "import os\n",
        "recording_files = os.listdir(recordings_path)\n",
        "print(\"Files in the recordings folder:\", recording_files)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
